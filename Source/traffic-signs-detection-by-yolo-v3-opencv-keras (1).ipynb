{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚õîÔ∏è Traffic Signs Detection with YOLO v3, OpenCV and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Firstly, trained model in Darknet framework **detects Traffic Signs among 4 categories** by OpenCV dnn library.\n",
    "* Then, trained model in Keras **classifies** cut fragmets of Traffic Signs into one of **43 classes**.\n",
    "* Results are experimental, but can be used for further improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© Related Papers\n",
    "\n",
    "1. Sichkar V. N., Kolyubin S. A. **Real time detection and classification of traffic signs based on YOLO version 3 algorithm.** *Scientific and Technical Journal of Information Technologies, Mechanics and Optics*, 2020, vol. 20, no. 3, pp. 418‚Äì424. DOI: 10.17586/2226-1494-2020-20-3-418-424 (Full-text available on ResearchGate here: [Real time detection and classification of traffic signs based on YOLO version 3 algorithm](https://www.researchgate.net/publication/342638954_Real_time_detection_and_classification_of_traffic_signs_based_on_YOLO_version_3_algorithm)\n",
    "\n",
    "1. Sichkar V. N., Kolyubin S. A. **Effect of various dimension convolutional layer filters on traffic sign classification accuracy.** *Scientific and Technical Journal of Information Technologies, Mechanics and Optics*, 2019, vol. 19, no. 3, pp. 546‚Äì552. DOI: 10.17586/2226-1494-2019-19-3-546-552 (Full-text available on ResearchGate here: [Effect of various dimension convolutional layer filters on traffic sign classification accuracy](https://www.researchgate.net/publication/334074308_Effect_of_various_dimension_convolutional_layer_filters_on_traffic_sign_classification_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì Related Course for Detection Tasks\n",
    "**Training YOLO v3 for Objects Detection with Custom Data.** *Build your own detector by labelling, training and testing on image, video and in real time with camera.* **Join here:** [https://www.udemy.com/course/training-yolo-v3-for-objects-detection-with-custom-data/](https://www.udemy.com/course/training-yolo-v3-for-objects-detection-with-custom-data/?referralCode=A283956A57327E37DDAD)\n",
    "\n",
    "Example of detections on video are shown below. **Trained weights** can be found in the course mentioned above.\n",
    "\n",
    "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3400968%2Fbcdae0b57021d6ac3e86a9aa2e8c4b08%2Fts_detections.gif?generation=1581700736851192&alt=media)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì• Importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['traffic-signs-classification-with-cnn', 'traffic-signs-dataset-in-yolo-format', 'traffic-signs-preprocessed', 'trained-traffic-signs-detector-based-on-yolo-v3']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import cv2\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('../input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "print(os.listdir('../input'))\n",
    "\n",
    "# Any results we write to the current directory are saved as output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÇ Loading *labels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ClassId                                           SignName\n",
      "0         0                               Speed limit (20km/h)\n",
      "1         1                               Speed limit (30km/h)\n",
      "2         2                               Speed limit (50km/h)\n",
      "3         3                               Speed limit (60km/h)\n",
      "4         4                               Speed limit (70km/h)\n",
      "5         5                               Speed limit (80km/h)\n",
      "6         6                        End of speed limit (80km/h)\n",
      "7         7                              Speed limit (100km/h)\n",
      "8         8                              Speed limit (120km/h)\n",
      "9         9                                         No passing\n",
      "10       10       No passing for vehicles over 3.5 metric tons\n",
      "11       11              Right-of-way at the next intersection\n",
      "12       12                                      Priority road\n",
      "13       13                                              Yield\n",
      "14       14                                               Stop\n",
      "15       15                                        No vehicles\n",
      "16       16           Vehicles over 3.5 metric tons prohibited\n",
      "17       17                                           No entry\n",
      "18       18                                    General caution\n",
      "19       19                        Dangerous curve to the left\n",
      "20       20                       Dangerous curve to the right\n",
      "21       21                                       Double curve\n",
      "22       22                                         Bumpy road\n",
      "23       23                                      Slippery road\n",
      "24       24                          Road narrows on the right\n",
      "25       25                                          Road work\n",
      "26       26                                    Traffic signals\n",
      "27       27                                        Pedestrians\n",
      "28       28                                  Children crossing\n",
      "29       29                                  Bicycles crossing\n",
      "30       30                                 Beware of ice/snow\n",
      "31       31                              Wild animals crossing\n",
      "32       32                End of all speed and passing limits\n",
      "33       33                                   Turn right ahead\n",
      "34       34                                    Turn left ahead\n",
      "35       35                                         Ahead only\n",
      "36       36                               Go straight or right\n",
      "37       37                                Go straight or left\n",
      "38       38                                         Keep right\n",
      "39       39                                          Keep left\n",
      "40       40                               Roundabout mandatory\n",
      "41       41                                  End of no passing\n",
      "42       42  End of no passing by vehicles over 3.5 metric ...\n",
      "\n",
      "Speed limit (20km/h)\n",
      "Speed limit (30km/h)\n"
     ]
    }
   ],
   "source": [
    "# Reading csv file with labels' names\n",
    "# Loading two columns [0, 1] into Pandas dataFrame\n",
    "labels = pd.read_csv('../input/traffic-signs-preprocessed/label_names.csv')\n",
    "\n",
    "# Check point\n",
    "# Showing first 5 rows from the dataFrame\n",
    "print(labels)\n",
    "print()\n",
    "\n",
    "# To locate by class number use one of the following\n",
    "# ***.iloc[0][1] - returns element on the 0 column and 1 row\n",
    "print(labels.iloc[0][1])  # Speed limit (20km/h)\n",
    "# ***['SignName'][1] - returns element on the column with name 'SignName' and 1 row\n",
    "print(labels['SignName'][1]) # Speed limit (30km/h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìç Loading trained Keras CNN model for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# Loading trained CNN model to use it later when classifying from 4 groups into one of 43 classes\n",
    "model = load_model('../input/traffic-signs-classification-with-cnn/model-23x23.h5')\n",
    "\n",
    "# Loading mean image to use for preprocessing further\n",
    "# Opening file for reading in binary mode\n",
    "with open('../input/traffic-signs-preprocessed/mean_image_rgb.pickle', 'rb') as f:\n",
    "    mean = pickle.load(f, encoding='latin1')  # dictionary type\n",
    "    \n",
    "print(mean['mean_image_rgb'].shape)  # (3, 32, 32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí† Loading YOLO v3 network by OpenCV dnn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading *trained weights* and *cfg file* into the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained weights can be found in the course mentioned above\n",
    "path_to_weights = '../input/trained-traffic-signs-detector-based-on-yolo-v3/yolov3_ts_train_5000.weights'\n",
    "path_to_cfg = '../input/traffic-signs-dataset-in-yolo-format/yolov3_ts_test.cfg'\n",
    "\n",
    "# Loading trained YOLO v3 weights and cfg configuration file by 'dnn' library from OpenCV\n",
    "network = cv2.dnn.readNetFromDarknet(path_to_cfg, path_to_weights)\n",
    "\n",
    "# To use with GPU\n",
    "network.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "network.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting *output layers* where detections are made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['yolo_82', 'yolo_94', 'yolo_106']\n"
     ]
    }
   ],
   "source": [
    "# Getting names of all YOLO v3 layers\n",
    "layers_all = network.getLayerNames()\n",
    "\n",
    "# Check point\n",
    "# print(layers_all)\n",
    "\n",
    "# Getting only detection YOLO v3 layers that are 82, 94 and 106\n",
    "layers_names_output = [layers_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]\n",
    "\n",
    "# Check point\n",
    "print()\n",
    "print(layers_names_output)  # ['yolo_82', 'yolo_94', 'yolo_106']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting *probability*, *threshold* and *colour* for bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(43, 3)\n",
      "[55 14 17]\n"
     ]
    }
   ],
   "source": [
    "# Minimum probability to eliminate weak detections\n",
    "probability_minimum = 0.2\n",
    "\n",
    "# Setting threshold to filtering weak bounding boxes by non-maximum suppression\n",
    "threshold = 0.2\n",
    "\n",
    "# Generating colours for bounding boxes\n",
    "# randint(low, high=None, size=None, dtype='l')\n",
    "colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n",
    "\n",
    "# Check point\n",
    "print(type(colours))  # <class 'numpy.ndarray'>\n",
    "print(colours.shape)  # (43, 3)\n",
    "print(colours[0])  # [25  65 200]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé¨ Reading input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading video from a file by VideoCapture object\n",
    "video = cv2.VideoCapture('../input/traffic-signs-dataset-in-yolo-format/traffic-sign-to-test.mp4')\n",
    "# video = cv2.VideoCapture('../input/videofortesting/ts_video_1.mp4')\n",
    "# video = cv2.VideoCapture('../input/videofortesting/ts_video_6.mp4')\n",
    "\n",
    "# Writer that will be used to write processed frames\n",
    "writer = None\n",
    "\n",
    "# Variables for spatial dimensions of the frames\n",
    "h, w = None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ûø Processing frames in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame number 1 took 0.37175 seconds\n",
      "Frame number 2 took 0.37181 seconds\n",
      "Frame number 3 took 0.38176 seconds\n",
      "Frame number 4 took 0.37899 seconds\n",
      "Frame number 5 took 0.38827 seconds\n",
      "Frame number 6 took 0.38110 seconds\n",
      "Frame number 7 took 0.38269 seconds\n",
      "Frame number 8 took 0.38547 seconds\n",
      "Frame number 9 took 0.38923 seconds\n",
      "Frame number 10 took 0.39311 seconds\n",
      "Frame number 11 took 0.39742 seconds\n",
      "Frame number 12 took 0.40619 seconds\n",
      "Frame number 13 took 0.40286 seconds\n",
      "Frame number 14 took 0.40671 seconds\n",
      "Frame number 15 took 0.45802 seconds\n",
      "Frame number 16 took 0.48687 seconds\n",
      "Frame number 17 took 0.46831 seconds\n",
      "Frame number 18 took 0.46719 seconds\n",
      "Frame number 19 took 0.46773 seconds\n",
      "Frame number 20 took 0.47314 seconds\n",
      "Frame number 21 took 0.49713 seconds\n",
      "Frame number 22 took 0.45568 seconds\n",
      "Frame number 23 took 0.49889 seconds\n",
      "Frame number 24 took 0.46123 seconds\n",
      "Frame number 25 took 0.46407 seconds\n",
      "Frame number 26 took 0.47990 seconds\n",
      "Frame number 27 took 0.46202 seconds\n",
      "Frame number 28 took 0.48233 seconds\n",
      "Frame number 29 took 0.46199 seconds\n",
      "Frame number 30 took 0.48299 seconds\n",
      "Frame number 31 took 0.45924 seconds\n",
      "Frame number 32 took 0.45630 seconds\n",
      "Frame number 33 took 0.47730 seconds\n",
      "Frame number 34 took 0.45464 seconds\n",
      "Frame number 35 took 0.47865 seconds\n",
      "Frame number 36 took 0.46944 seconds\n",
      "Frame number 37 took 0.48863 seconds\n",
      "Frame number 38 took 0.45739 seconds\n",
      "Frame number 39 took 0.46729 seconds\n",
      "Frame number 40 took 0.47186 seconds\n",
      "Frame number 41 took 0.47642 seconds\n",
      "Frame number 42 took 0.47649 seconds\n",
      "Frame number 43 took 0.50211 seconds\n",
      "Frame number 44 took 0.47754 seconds\n",
      "Frame number 45 took 0.46557 seconds\n",
      "Frame number 46 took 0.46343 seconds\n",
      "Frame number 47 took 0.47201 seconds\n",
      "Frame number 48 took 0.46009 seconds\n",
      "Frame number 49 took 0.48682 seconds\n",
      "Frame number 50 took 0.45577 seconds\n",
      "Frame number 51 took 0.49774 seconds\n",
      "Frame number 52 took 0.51828 seconds\n",
      "Frame number 53 took 0.53641 seconds\n",
      "Frame number 54 took 0.57566 seconds\n",
      "Frame number 55 took 0.54284 seconds\n",
      "Frame number 56 took 0.50096 seconds\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Setting default size of plots\n",
    "plt.rcParams['figure.figsize'] = (3, 3)\n",
    "\n",
    "# Variable for counting total amount of frames\n",
    "f = 0\n",
    "\n",
    "# Variable for counting total processing time\n",
    "t = 0\n",
    "\n",
    "# Catching frames in the loop\n",
    "while True:\n",
    "    # Capturing frames one-by-one\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    # If the frame was not retrieved\n",
    "    if not ret:\n",
    "        break\n",
    "       \n",
    "    # Getting spatial dimensions of the frame for the first time\n",
    "    if w is None or h is None:\n",
    "        # Slicing two elements from tuple\n",
    "        h, w = frame.shape[:2]\n",
    "\n",
    "    # Blob from current frame\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "\n",
    "    # Forward pass with blob through output layers\n",
    "    network.setInput(blob)\n",
    "    start = time.time()\n",
    "    output_from_network = network.forward(layers_names_output)\n",
    "    end = time.time()\n",
    "\n",
    "    # Increasing counters\n",
    "    f += 1\n",
    "    t += end - start\n",
    "\n",
    "    # Spent time for current frame\n",
    "    print('Frame number {0} took {1:.5f} seconds'.format(f, end - start))\n",
    "\n",
    "    # Lists for detected bounding boxes, confidences and class's number\n",
    "    bounding_boxes = []\n",
    "    confidences = []\n",
    "    class_numbers = []\n",
    "\n",
    "    # Going through all output layers after feed forward pass\n",
    "    for result in output_from_network:\n",
    "        # Going through all detections from current output layer\n",
    "        for detected_objects in result:\n",
    "            # Getting 80 classes' probabilities for current detected object\n",
    "            scores = detected_objects[5:]\n",
    "            # Getting index of the class with the maximum value of probability\n",
    "            class_current = np.argmax(scores)\n",
    "            # Getting value of probability for defined class\n",
    "            confidence_current = scores[class_current]\n",
    "\n",
    "            # Eliminating weak predictions by minimum probability\n",
    "            if confidence_current > probability_minimum:\n",
    "                # Scaling bounding box coordinates to the initial frame size\n",
    "                box_current = detected_objects[0:4] * np.array([w, h, w, h])\n",
    "\n",
    "                # Getting top left corner coordinates\n",
    "                x_center, y_center, box_width, box_height = box_current\n",
    "                x_min = int(x_center - (box_width / 2))\n",
    "                y_min = int(y_center - (box_height / 2))\n",
    "\n",
    "                # Adding results into prepared lists\n",
    "                bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n",
    "                confidences.append(float(confidence_current))\n",
    "                class_numbers.append(class_current)\n",
    "                \n",
    "\n",
    "    # Implementing non-maximum suppression of given bounding boxes\n",
    "    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n",
    "\n",
    "    # Checking if there is any detected object been left\n",
    "    if len(results) > 0:\n",
    "        # Going through indexes of results\n",
    "        for i in results.flatten():\n",
    "            # Bounding box coordinates, its width and height\n",
    "            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n",
    "            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n",
    "            \n",
    "            \n",
    "            # Cut fragment with Traffic Sign\n",
    "            c_ts = frame[y_min:y_min+int(box_height), x_min:x_min+int(box_width), :]\n",
    "            # print(c_ts.shape)\n",
    "            \n",
    "            if c_ts.shape[:1] == (0,) or c_ts.shape[1:2] == (0,):\n",
    "                pass\n",
    "            else:\n",
    "                # Getting preprocessed blob with Traffic Sign of needed shape\n",
    "                blob_ts = cv2.dnn.blobFromImage(c_ts, 1 / 255.0, size=(32, 32), swapRB=True, crop=False)\n",
    "                blob_ts[0] = blob_ts[0, :, :, :] - mean['mean_image_rgb']\n",
    "                blob_ts = blob_ts.transpose(0, 2, 3, 1)\n",
    "                # plt.imshow(blob_ts[0, :, :, :])\n",
    "                # plt.show()\n",
    "\n",
    "                # Feeding to the Keras CNN model to get predicted label among 43 classes\n",
    "                scores = model.predict(blob_ts)\n",
    "\n",
    "                # Scores is given for image with 43 numbers of predictions for each class\n",
    "                # Getting only one class with maximum value\n",
    "                prediction = np.argmax(scores)\n",
    "                # print(labels['SignName'][prediction])\n",
    "\n",
    "\n",
    "                # Colour for current bounding box\n",
    "                colour_box_current = colours[class_numbers[i]].tolist()\n",
    "                colour_box_current = [1, 94, 94]\n",
    "                # Drawing bounding box on the original current frame\n",
    "                cv2.rectangle(frame, (x_min, y_min),\n",
    "                              (x_min + box_width, y_min + box_height),\n",
    "                              colour_box_current, 2)\n",
    "\n",
    "                # Preparing text with label and confidence for current bounding box\n",
    "                text_box_current = '{}: {:.4f}'.format(labels['SignName'][prediction],\n",
    "                                                       confidences[i])\n",
    "\n",
    "                # Putting text with label and confidence on the original image\n",
    "                cv2.putText(frame, text_box_current, (x_min, y_min - 5),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)\n",
    "\n",
    "\n",
    "    # Initializing writer only once\n",
    "    if writer is None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "        # Writing current processed frame into the video file\n",
    "        writer = cv2.VideoWriter('result.mp4', fourcc, 25,\n",
    "                                 (frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "    # Write processed current frame to the file\n",
    "    writer.write(frame)\n",
    "\n",
    "\n",
    "# Releasing video reader and writer\n",
    "video.release()\n",
    "writer.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ FPS results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frames 56\n",
      "Total amount of time 25.59373 seconds\n",
      "FPS: 2.2\n"
     ]
    }
   ],
   "source": [
    "print('Total number of frames', f)\n",
    "print('Total amount of time {:.5f} seconds'.format(t))\n",
    "print('FPS:', round((f / t), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='result.mp4' target='_blank'>result.mp4</a><br>"
      ],
      "text/plain": [
       "C:\\Users\\chautej\\Desktop\\Coursera\\Traffic Sign Recognition\\Source\\result.mp4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving locally without committing\n",
    "from IPython.display import FileLink\n",
    "\n",
    "FileLink('result.mp4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-c3821804bbee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Check point\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Showing image shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Image shape:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage_BGR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# tuple of (731, 1092, 3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Getting spatial dimension of input image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Reading image with OpenCV library\n",
    "# In this way image is opened already as numpy array\n",
    "# WARNING! OpenCV by default reads images in BGR format\n",
    "# image_BGR = cv2.imread('../input/videofortesting/traffic_sign.jpg')\n",
    "# image_BGR = cv2.imread('../input/videofortesting/ts_video_1_1.png')\n",
    "# image_BGR = cv2.imread('../input/videofortesting/ts_video_1_2.png')\n",
    "# image_BGR = cv2.imread('../input/videofortesting/ts_video_1_3.png')\n",
    "# image_BGR = cv2.imread('../input/videofortesting/ts_video_1_4.png')\n",
    "# image_BGR = cv2.imread('../input/videofortesting/ts_video_6_1.png')\n",
    "# image_BGR = cv2.imread('../input/videofortesting/ts_video_6_2.png')\n",
    "# image_BGR = cv2.imread('../input/videofortesting/ts_video_6_3.png')\n",
    "image_BGR = cv2.imread('../input/videofortesting/ts_final_1.png')\n",
    "\n",
    "# Check point\n",
    "# Showing image shape\n",
    "print('Image shape:', image_BGR.shape)  # tuple of (731, 1092, 3)\n",
    "\n",
    "# Getting spatial dimension of input image\n",
    "h, w = image_BGR.shape[:2]  # Slicing from tuple only first two elements\n",
    "\n",
    "# Check point\n",
    "# Showing height an width of image\n",
    "print('Image height={0} and width={1}'.format(h, w))  # 731 1092\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable for counting total processing time\n",
    "t = 0\n",
    "\n",
    "# Blob from current frame\n",
    "blob = cv2.dnn.blobFromImage(image_BGR, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "\n",
    "# Forward pass with blob through output layers\n",
    "network.setInput(blob)\n",
    "start = time.time()\n",
    "output_from_network = network.forward(layers_names_output)\n",
    "end = time.time()\n",
    "\n",
    "# Time\n",
    "t += end - start\n",
    "print('Total amount of time {:.5f} seconds'.format(t))\n",
    "\n",
    "# Lists for detected bounding boxes, confidences and class's number\n",
    "bounding_boxes = []\n",
    "confidences = []\n",
    "class_numbers = []\n",
    "\n",
    "# Going through all output layers after feed forward pass\n",
    "for result in output_from_network:\n",
    "    # Going through all detections from current output layer\n",
    "    for detected_objects in result:\n",
    "        # Getting 80 classes' probabilities for current detected object\n",
    "        scores = detected_objects[5:]\n",
    "        # Getting index of the class with the maximum value of probability\n",
    "        class_current = np.argmax(scores)\n",
    "        # Getting value of probability for defined class\n",
    "        confidence_current = scores[class_current]\n",
    "\n",
    "        # Eliminating weak predictions by minimum probability\n",
    "        if confidence_current > probability_minimum:\n",
    "            # Scaling bounding box coordinates to the initial frame size\n",
    "            box_current = detected_objects[0:4] * np.array([w, h, w, h])\n",
    "\n",
    "            # Getting top left corner coordinates\n",
    "            x_center, y_center, box_width, box_height = box_current\n",
    "            x_min = int(x_center - (box_width / 2))\n",
    "            y_min = int(y_center - (box_height / 2))\n",
    "\n",
    "            # Adding results into prepared lists\n",
    "            bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n",
    "            confidences.append(float(confidence_current))\n",
    "            class_numbers.append(class_current)\n",
    "                \n",
    "\n",
    "# Implementing non-maximum suppression of given bounding boxes\n",
    "results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n",
    "\n",
    "# Checking if there is any detected object been left\n",
    "if len(results) > 0:\n",
    "    # Going through indexes of results\n",
    "    for i in results.flatten():\n",
    "        # Bounding box coordinates, its width and height\n",
    "        x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n",
    "        box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n",
    "            \n",
    "            \n",
    "        # Cut fragment with Traffic Sign\n",
    "        c_ts = image_BGR[y_min:y_min+int(box_height), x_min:x_min+int(box_width), :]\n",
    "        # print(c_ts.shape)\n",
    "            \n",
    "        if c_ts.shape[:1] == (0,) or c_ts.shape[1:2] == (0,):\n",
    "            pass\n",
    "        else:\n",
    "            # Getting preprocessed blob with Traffic Sign of needed shape\n",
    "            blob_ts = cv2.dnn.blobFromImage(c_ts, 1 / 255.0, size=(32, 32), swapRB=True, crop=False)\n",
    "            blob_ts[0] = blob_ts[0, :, :, :] - mean['mean_image_rgb']\n",
    "            blob_ts = blob_ts.transpose(0, 2, 3, 1)\n",
    "            # plt.imshow(blob_ts[0, :, :, :])\n",
    "            # plt.show()\n",
    "\n",
    "            # Feeding to the Keras CNN model to get predicted label among 43 classes\n",
    "            scores = model.predict(blob_ts)\n",
    "\n",
    "            # Scores is given for image with 43 numbers of predictions for each class\n",
    "            # Getting only one class with maximum value\n",
    "            prediction = np.argmax(scores)\n",
    "            print(labels['SignName'][prediction])\n",
    "\n",
    "\n",
    "            # Colour for current bounding box\n",
    "            colour_box_current = colours[class_numbers[i]].tolist()\n",
    "            \n",
    "            # Green BGR\n",
    "            colour_box_current = [0, 255, 61]\n",
    "            \n",
    "            # Yellow BGR\n",
    "#             colour_box_current = [0, 255, 255]\n",
    "\n",
    "            # Drawing bounding box on the original current frame\n",
    "            cv2.rectangle(image_BGR, (x_min, y_min),\n",
    "                              (x_min + box_width, y_min + box_height),\n",
    "                              colour_box_current, 6)\n",
    "\n",
    "#             # Preparing text with label and confidence for current bounding box\n",
    "#             text_box_current = '{}: {:.4f}'.format(labels['SignName'][prediction],\n",
    "#                                                    confidences[i])\n",
    "            \n",
    "#             # Putting text with label and confidence on the original image\n",
    "#             cv2.putText(image_BGR, text_box_current, (x_min, y_min - 15),\n",
    "#                             cv2.FONT_HERSHEY_SIMPLEX, 0.9, colour_box_current, 2)\n",
    "            \n",
    "            if prediction == 5:\n",
    "                # Preparing text with label and confidence for current bounding box\n",
    "                text_box_current = '{}: {:.4f}'.format('Speed limit 60', confidences[i])\n",
    "\n",
    "                # Putting text with label and confidence on the original image\n",
    "                cv2.putText(image_BGR, text_box_current, (x_min - 110, y_min - 10),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, colour_box_current, 2)\n",
    "                \n",
    "            elif prediction == 9:            \n",
    "                # Preparing text with label and confidence for current bounding box\n",
    "                text_box_current = '{}: {:.4f}'.format('No overtaking', confidences[i])\n",
    "\n",
    "                # Putting text with label and confidence on the original image\n",
    "                cv2.putText(image_BGR, text_box_current, (x_min - 110, y_min + box_height + 30),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, colour_box_current, 2)\n",
    "\n",
    "#             elif prediction == 17:            \n",
    "#                 # Preparing text with label and confidence for current bounding box\n",
    "#                 text_box_current = '{}: {:.4f}'.format('No entry', confidences[i])\n",
    "\n",
    "#                 # Putting text with label and confidence on the original image\n",
    "#                 cv2.putText(image_BGR, text_box_current, (x_min - 170, y_min - 15),\n",
    "#                                 cv2.FONT_HERSHEY_SIMPLEX, 0.9, colour_box_current, 2)\n",
    "                \n",
    "                \n",
    "# Saving image\n",
    "cv2.imwrite('result.png', image_BGR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing processed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (35.0, 35.0) # Setting default size of plots\n",
    "\n",
    "image_BGR = cv2.imread('/kaggle/working/result.png')\n",
    "\n",
    "# Showing image shape\n",
    "print('Image shape:', image_BGR.shape)  # tuple of (800, 1360, 3)\n",
    "\n",
    "# Getting spatial dimension of input image\n",
    "h, w = image_BGR.shape[:2]  # Slicing from tuple only first two elements\n",
    "\n",
    "# Showing height an width of image\n",
    "print('Image height={0} and width={1}'.format(h, w))  # 800 1360\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image_BGR, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "# plt.title('Keras Visualization', fontsize=18)\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n",
    "\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving locally without committing\n",
    "from IPython.display import FileLink\n",
    "\n",
    "FileLink('result.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîé Example of the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3400968%2Fa57f58b38e3caab6fbf72169895f5074%2Fresult.gif?generation=1585955236302060&alt=media)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
